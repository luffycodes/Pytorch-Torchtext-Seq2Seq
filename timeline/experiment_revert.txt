experiment:
noisy initial state in encoder & decoder:
*** enc_h_0 = Variable(src_embed.data.new(*h_size).normal_(), requires_grad=False) - Do this !
now it is back to zero_()

experiment:
h_size = (self.num_layers, batch_size, self.hidden_dim)
vs
h_size = (self.num_layers * 2, batch_size, self.hidden_dim)

experiment:
switched off logging in validation

experiment:
changing the loss function:
from :
        loss = torch.mm(bi_enc_h_t, bi_dec_h_t.transpose(0, 1))
        for x in range(0, loss.size()[0]):
            loss[x, x] = - 10 * loss[x, x]
        loss = torch.sum(torch.sigmoid(loss))
to:
        loss = torch.mm(bi_enc_h_t, bi_dec_h_t.transpose(0, 1))
        loss = -1 * loss
        for x in range(0, loss.size()[0]):
            loss[x, x] = - loss[x, x]
        loss = torch.sum(torch.log(torch.sigmoid(loss)))
        loss = -1 * loss

experiment: (reverted)
from:
        bi_enc_h_t = torch.sum(enc_h_t, dim=1)
        bi_dec_h_t = torch.sum(dec_h_t, dim=1)

to:
        bi_enc_h_t = torch.sigmoid(torch.sum(enc_h_t, dim=1))
        bi_dec_h_t = torch.sigmoid(torch.sum(dec_h_t, dim=1))

experiment: (reverted)
from:
        bi_enc_h_t = torch.sum(enc_h_t, dim=1)
        bi_dec_h_t = torch.sum(dec_h_t, dim=1)
        _
        _
to:
        bi_enc_h_t = bi_enc_h_t.div(bi_enc_h_t.norm(p=2, dim=1, keepdim=True).expand_as(bi_enc_h_t))
        bi_dec_h_t = bi_dec_h_t.div(bi_dec_h_t.norm(p=2, dim=1, keepdim=True).expand_as(bi_dec_h_t))

experiment:
        self.embedding = nn.Embedding(vocab_size, embed_dim, max_norm=1)

experiment:
        self.optimizer = optim.Adam(self.model.parameters(), weight_decay=0.2)
        reduced to 0.001: as sum of weights was not growing

